# Experiment Infrastructure Plan for IDMR (2025-01-18)

## Overview

Run Tables I, II, III simulations on AWS within 1 month, budget $1000.
- Table I: Large-d IDC performance (CPU)
- Table II: SGD comparison (GPU)
- Table III: L1-regularized IDC (CPU)

---

## Phase 1: Pre-requisites (Code Changes)

### 1.1 Wire `n_workers` Parameter

Problem: `IDCConfig.n_workers` is defined but NOT forwarded to `ProcessPoolExecutor`.

Files to modify:
- `PB.py` (MDR_v11.__init__): add `n_workers`
- `PB.py` (`PARALLEL_initialize_theta_PairwiseBinomial`, `PARALLEL_update_theta_bar_mat`): pass `max_workers=self.n_workers`
- `idmr_core/models.py`: forward `n_workers` when creating engine

Why needed: Prevents core oversubscription when running multiple jobs on the same machine.

### 1.2 (Optional) Wire Solver Selection

Problem: `poisson_solver` config is ignored; hardcoded to `SCS`.

Files to modify:
- `classesv2.py`: use passed solver instead of hardcoded `SCS`
- `PB.py`: forward solver parameter through calls

Why helpful: MOSEK is faster but requires a license. Can skip for now.

---

## Phase 2: Experiment Runner Script

### 2.1 Create `scripts/run_experiments.py`

Single script that can run any table's experiments:

```bash
python scripts/run_experiments.py \
    --table 1 \
    --dgp A \
    --d 250 500 1000 \
    --n 1000 \
    --S 10 20 \
    --B 10 \
    --partition 0 \
    --n-partitions 4 \
    --output-dir s3://bucket/results/ \
    --n-workers 8
```

Features:
- Partition-aware: `--partition` and `--n-partitions` for multi-machine split
- Saves results as CSV per (table, dgp, d, method) combination
- Checkpointing: skip completed seeds (resume after interruption)
- Progress logging to stdout (for monitoring)

### 2.2 Output Format

```
results/
  table1/
    dgp_A_d_250_S_10.csv    # columns: seed, mse, bias, variance, time
    dgp_A_d_500_S_10.csv
  table2/
    dgp_A_adam_lr_0.01.csv
  table3/
    ...
```

### 2.3 Aggregation Script

`scripts/aggregate_results.py`: combine CSVs into final tables for paper.

---

## Phase 3: AWS Infrastructure

### 3.1 Instance Selection (Spot, $1000 budget)

| Pool  | Instance    | vCPUs | $/hr (Spot) | Purpose           |
|-------|-------------|-------|-------------|-------------------|
| CPU-1 | c6i.4xlarge | 16    | ~0.25       | Table I, DGP-A     |
| CPU-2 | c6i.4xlarge | 16    | ~0.25       | Table I, DGP-C     |
| CPU-3 | c6i.4xlarge | 16    | ~0.25       | Table III          |
| GPU-1 | g5.xlarge   | 4     | ~0.35       | Table II (SGD)     |

Total: ~1.10/hr * 720 hrs = ~$800/month (within budget).

### 3.2 Setup Script: `scripts/aws_setup.sh`

```bash
#!/bin/bash
# Run on each EC2 instance after launch

# Install uv and dependencies
curl -LsSf https://astral.sh/uv/install.sh | sh
git clone <repo> ~/IDMR
cd ~/IDMR
uv sync

# Create tmux session
tmux new-session -d -s exp

# AWS CLI for S3 (pre-installed on Amazon Linux)
aws configure set default.region us-east-1
```

### 3.3 Launch Script: `scripts/launch_experiments.sh`

```bash
#!/bin/bash
# Launch experiments in tmux

# Machine 1: Table I, DGP-A
tmux send-keys -t exp "python scripts/run_experiments.py --table 1 --dgp A --d 250 500 1000 2000 5000 --B 50 --output-dir s3://bucket/table1/" Enter

# Machine 2: Table I, DGP-C
# ... similar

# Machine 3: Table III
# ...

# Machine 4: Table II (GPU)
# ...
```

### 3.4 S3 Bucket Structure

```
s3://idmr-experiments/
  table1/
  table2/
  table3/
  logs/
```

---

## Phase 4: Parallelization Strategy

### 4.1 Machine Assignment

| Machine | Table | DGP | d values                 | Est. runs |
|---------|-------|-----|--------------------------|-----------|
| CPU-1   | I     | A   | 250, 500, 1000, 2000, 5000 | 5d x 2S x 10B = 100 (or 500 if B=50) |
| CPU-2   | I     | C   | 250, 500, 1000, 2000, 5000 | 5d x 2S x 10B = 100 (or 500 if B=50) |
| CPU-3   | III   | A   | 5d x 5p x 3lambda         | 75 x 10 = 750 (or 3750 if B=50) |
| GPU-1   | II    | A,C | All SGD configs           | 2dgp x 3opt x 3lr x 5d x 10B = 900 |

### 4.2 Within-Machine Parallelization

- IDC jobs: use `n_workers=12` (leave 4 cores for the OS)
- SGD jobs: run on GPU, tune batch size for memory

### 4.3 Redundancy Strategy

Spot instance reclaim handling:
1. Checkpoint after each seed
2. Check for existing results before starting
3. If reclaimed, restart on new instance and resume

---

## Phase 5: Execution Plan

### Week 1: Code Changes + Local Testing
1. Wire `n_workers` parameter
2. Create experiment runner script
3. Test locally with d=250, B=5

### Week 2: AWS Setup + Small Scale
1. Launch 1 CPU instance
2. Test full pipeline with d=250, B=10
3. Verify S3 upload works
4. Fix any issues

### Week 3-4: Full Scale Execution
1. Launch all 4 instances
2. Start experiments
3. Monitor via `tail -f` on tmux
4. Aggregate results as they complete

---

## Files to Create/Modify

### New Files
1. `scripts/run_experiments.py` - Main experiment runner
2. `scripts/aggregate_results.py` - Combine CSVs into tables
3. `scripts/aws_setup.sh` - Instance setup script
4. `scripts/launch_experiments.sh` - tmux launch commands

### Modified Files
1. `PB.py` - Wire `n_workers` to `ProcessPoolExecutor`
2. `idmr_core/models.py` - Forward `n_workers` to engine

---

## Verification

### Local Testing

```bash
# Test experiment runner
python scripts/run_experiments.py --table 1 --dgp A --d 250 --B 2 --output-dir ./test_results/

# Verify output CSV exists and has correct format
cat test_results/table1/dgp_A_d_250_S_10.csv
```

### AWS Testing

```bash
# SSH and check progress
ssh -i key.pem ec2-user@<ip> "tmux attach -t exp"

# Check S3 for results
aws s3 ls s3://idmr-experiments/table1/
```

---

## Decisions Made

1. AWS region: us-east-1 (best spot pricing)
2. Lambda values for Table III: lambda in {0, 0.01, 0.1} (unpenalized + two L1 strengths)
3. Repetitions: B=10 first, scale to 50 if time/budget permits
4. S3 bucket: `idmr-experiments-2025` (or similar unique name)
